{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norwegian dataset splitting and augmentation\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import augmentation\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import augment_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Image      Word\n",
      "0       no-nb_digimanus_101261_0001_0.jpg       Til\n",
      "1       no-nb_digimanus_101261_0001_1.jpg  assessor\n",
      "2      no-nb_digimanus_101261_0001_10.jpg     takke\n",
      "3      no-nb_digimanus_101261_0001_11.jpg       Dig\n",
      "4      no-nb_digimanus_101261_0001_12.jpg        på\n",
      "...                                   ...       ...\n",
      "46586   no-nb_digimanus_81638_0001_60.jpg  hengivne\n",
      "46587   no-nb_digimanus_81638_0001_61.jpg   Harriet\n",
      "46588    no-nb_digimanus_81638_0001_7.jpg       Tak\n",
      "46589    no-nb_digimanus_81638_0001_8.jpg       for\n",
      "46590    no-nb_digimanus_81638_0001_9.jpg      hvad\n",
      "\n",
      "[46591 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../image_data/norwegian_data/trim.csv')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified K-fold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/D1/projects/ZeroShot_Word_Recognition/Transformer_ZeroShot_Word_Recognition/joakims_work/myphosc/venv/lib/python3.9/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "100%|██████████| 33544/33544 [10:57<00:00, 51.02it/s]\n",
      "100%|██████████| 3728/3728 [01:11<00:00, 51.86it/s]\n",
      "100%|██████████| 9319/9319 [03:01<00:00, 51.31it/s]\n",
      "100%|██████████| 33545/33545 [10:49<00:00, 51.65it/s] \n",
      "100%|██████████| 3728/3728 [01:13<00:00, 50.80it/s]\n",
      "100%|██████████| 9318/9318 [03:03<00:00, 50.65it/s]\n",
      "100%|██████████| 33545/33545 [10:41<00:00, 52.29it/s]\n",
      "100%|██████████| 3728/3728 [01:15<00:00, 49.11it/s]\n",
      "100%|██████████| 9318/9318 [03:04<00:00, 50.52it/s]\n",
      "100%|██████████| 33545/33545 [10:59<00:00, 50.86it/s]\n",
      "100%|██████████| 3728/3728 [01:15<00:00, 49.21it/s]\n",
      "100%|██████████| 9318/9318 [03:10<00:00, 48.83it/s]\n",
      "100%|██████████| 33545/33545 [10:44<00:00, 52.07it/s]\n",
      "100%|██████████| 3728/3728 [01:13<00:00, 50.89it/s]\n",
      "100%|██████████| 9318/9318 [03:01<00:00, 51.25it/s]\n"
     ]
    }
   ],
   "source": [
    "def copy_split_to_folder(df: pd.DataFrame, src_path: str, dest_path: str):\n",
    "    os.makedirs(dest_path)\n",
    "\n",
    "    for img_name in tqdm(df['Image']):\n",
    "        img_path = os.path.join(src_path, img_name)\n",
    "        new_img_path = os.path.join(dest_path, img_name)\n",
    "\n",
    "        img = cv.imread(img_path)\n",
    "        img = augmentation.resize_img(img)\n",
    "        img = augmentation.gray_scale_img(img)\n",
    "        img = augmentation.threshold_image(img)\n",
    "\n",
    "        cv.imwrite(new_img_path, img)\n",
    "\n",
    "\n",
    "k_folder = StratifiedKFold(5, shuffle=True)\n",
    "\n",
    "split_index = k_folder.split(df['Image'], df['Word'])\n",
    "\n",
    "c = 1\n",
    "for train_index, test_index in split_index:\n",
    "    # print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "\n",
    "    train_fold = df.iloc[train_index]\n",
    "    test_fold = df.iloc[test_index]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        train_fold['Image'], train_fold['Word'], test_size=0.1)\n",
    "\n",
    "    df_train = pd.concat([X_train, y_train], axis=1)\n",
    "    df_valid = pd.concat([X_valid, y_valid], axis=1)\n",
    "    df_test = test_fold\n",
    "\n",
    "    # print('TRAIN SIZE:', len(df_train))\n",
    "    # print('VALID SIZE:', len(df_valid))\n",
    "    # print('TEST SIZE:', len(df_test))\n",
    "\n",
    "    df_train.to_csv(\n",
    "        f'../image_data/norwegian_data/train_threshold_split{c}.csv', index=False)\n",
    "    df_valid.to_csv(\n",
    "        f'../image_data/norwegian_data/valid_threshold_split{c}.csv', index=False)\n",
    "    df_test.to_csv(\n",
    "        f'../image_data/norwegian_data/test_threshold_split{c}.csv', index=False)\n",
    "\n",
    "    copy_split_to_folder(df_train, '../image_data/norwegian_data/trim',\n",
    "                         f'../image_data/norwegian_data/train_threshold_split{c}')\n",
    "    copy_split_to_folder(df_valid, '../image_data/norwegian_data/trim',\n",
    "                         f'../image_data/norwegian_data/valid_threshold_split{c}')\n",
    "    copy_split_to_folder(df_test, '../image_data/norwegian_data/trim',\n",
    "                         f'../image_data/norwegian_data/test_threshold_split{c}')\n",
    "\n",
    "    c += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation of train splits (use augment_dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 6):\n",
    "#     src_path = f'image_data/norwegian_data/train_split{i}'\n",
    "#     df = pd.read_csv(f'../image_data/norwegian_data/test_split{i}.csv')\n",
    "\n",
    "#     words_count = augment_dataset.count_words(df)\n",
    "\n",
    "#     print(len(words_count))\n",
    "\n",
    "    # for word in tqdm(words_count.keys()):\n",
    "    #     df = augment_dataset.augment_word_class(\n",
    "    #         df, word, words_count[word], src_path, src_path, 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7585dd4ff129a3d41bb6c9e690265866f8901fe8c84f068f677d959cb33c37c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
